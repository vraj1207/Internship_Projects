{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location, company_name,\n",
    "experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore”\n",
    "in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer starts from below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "search_field_location.send_keys(\"Bangalore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Immediate opening For Data Scientist/Data Analyst', 'Data Analyst']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        job_title.append(\"--\") \n",
    "    else:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "job_title[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chennai, Pune, Bengaluru, Hyderabad', 'Chennai, Delhi NCR, Bengaluru']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-location\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in locations:\n",
    "    if i.text is None :\n",
    "        job_location.append(\"--\") \n",
    "    else:\n",
    "        job_location.append(i.text)\n",
    "job_location[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAIA-Center For Artificial Intelligence & Advanced Analytics',\n",
       " 'Hk solutions']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the company_name \n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\") \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "company_name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"27f96f05b4d2f1d26d4829d75628d6c7\", element=\"dbbe663f-4d93-4491-b51c-d4f2e5aa59ed\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"27f96f05b4d2f1d26d4829d75628d6c7\", element=\"fa169f62-2ca9-472a-b33a-98827ccf34e2\")>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #scraping the experience_required \n",
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience:\n",
    "    if i.text is None :\n",
    "            experience_required.append(\"--\") \n",
    "    else:\n",
    "            experience_required.append(i.text)\n",
    "experience_required[0:2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_required</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>(2533 Reviews)</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_required                                       company_name  \\\n",
       "0             0-3 Yrs  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1             0-3 Yrs                                       Hk solutions   \n",
       "2             0-5 Yrs        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "3             2-6 Yrs                  Flipkart Internet Private Limited   \n",
       "4             3-8 Yrs                                     (2533 Reviews)   \n",
       "\n",
       "                          job_location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1        Chennai, Delhi NCR, Bengaluru   \n",
       "2                            Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "\n",
       "                                           job_title  \n",
       "0  Immediate opening For Data Scientist/Data Analyst  \n",
       "1                                       Data Analyst  \n",
       "2                          Intern - DFM Data Analyst  \n",
       "3  Hiring Data Analysts on Contract Third party p...  \n",
       "4                           Reliability Data Analyst  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"experience_required\":experience_required[0:10],\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_title\":job_title[0:10]})\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done\n",
    "manually.\n",
    "WEB SCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_field_location.send_keys(\"Bangalore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "full_job_description=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Immediate opening For Data Scientist/Data Analyst', 'Data Analyst']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        job_title.append(\"--\") \n",
    "    else:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "job_title[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAIA-Center For Artificial Intelligence & Advanced Analytics',\n",
       " 'Hk solutions']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the company_name \n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\") \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "company_name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chennai, Pune, Bengaluru, Hyderabad', 'Chennai, Delhi NCR, Bengaluru']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-location\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in locations:\n",
    "    if i.text is None :\n",
    "        job_location.append(\"--\") \n",
    "    else:\n",
    "        job_location.append(i.text)\n",
    "job_location[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping the full job-description, for scraping full job description we have to go in each of the jobs separately\n",
    "urls=[i.get_attribute(\"href\")for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")]\n",
    "for url in urls[0:10]:\n",
    "    try:\n",
    "        \n",
    "        driver.get(url)\n",
    "        raw_description=driver.find_element_by_xpath(\"//section[@class='job-desc']/div[1]\").text\n",
    "        description=raw_description.replace(\"Contact Person\",\"@@@@@\")\n",
    "        description= description.split(\"@@@@@\")\n",
    "        job_description.append(description[0])\n",
    "    except NoSuchElementException :\n",
    "        job_description.append(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>Dear Candidate\\n\\nSchedule a Telephonic Interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>About Data Sciences at Flipkart:\\nThe Data Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Experience Required: 0 - 6 Years.\\nWe are look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>(2537 Reviews)</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Sr. Data Scientist (Min 6 yrs in Data Science/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                                       Data Analyst   \n",
       "2                          Intern - DFM Data Analyst   \n",
       "3  Hiring Data Analysts on Contract Third party p...   \n",
       "4                           Reliability Data Analyst   \n",
       "\n",
       "                                        company_name  \\\n",
       "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1                                       Hk solutions   \n",
       "2        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "3                  Flipkart Internet Private Limited   \n",
       "4                                     (2537 Reviews)   \n",
       "\n",
       "                          job_location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1        Chennai, Delhi NCR, Bengaluru   \n",
       "2                            Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "\n",
       "                                     job_description  \n",
       "0  Dear Candidate\\n\\nSchedule a Telephonic Interv...  \n",
       "1  About Data Sciences at Flipkart:\\nThe Data Sci...  \n",
       "2  Experience Required: 0 - 6 Years.\\nWe are look...  \n",
       "3                                                ---  \n",
       "4  Sr. Data Scientist (Min 6 yrs in Data Science/...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"job_title\":job_title[0:10],\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_description\":job_description[0:10]})\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear Candidate\\n\\nSchedule a Telephonic Interview ( Please call for confirmation) : Mon- Sat from 11:00am to 5:00pm\\n\\nOR Walk-In to the Corporate office between Monday to Friday from 11:00am to 5:00pm (Date : 19th October onwards)\\nContact person :\\n\\nManigandan -+91 7299917200\\nShantha +91 9790993237\\n\\n\\nSystech Solutions\\nTemple Steps\\nTower 3, 6th Floor\\n184-187 Anna Salai\\nSaidapet, Chennai 600015\\nIndia (Near Little Mount Metro Station)\\nRoles and Responsibilities\\n\\nGreetings from CAIA !\\nA great opportunity to enter the world of future technologies - Data Science, Analytics, AI, Data Visualization\\nApplications invited from all Freshers and experienced candidates aspiring to make a career in Artificial Intelligence and Advanced Analytics and Data Science.\\nIn case you are trying to shift your career to Analytics and/or AI domain please do connect with us to know more.\\nWhat is needed from you?\\n- An Educational background in any one of the following- BE/B.Tech, ME/M Tech, MSc, BSc/MSc Maths and Statistics, B Com, BCA, BSc CS, BSC IT, MSC IT, MCA\\n- Skills relating to Mathematics/Statistics.\\n- Natural passion towards numbers, business, coding, Analytics and Artificial Intelligence, Machine Learning, visualization\\n- Good verbal and written communication skills\\n- Ability to understand domains in businesses across various sectors\\n- Freshers who wish to start their career in Analytics and AI and professionals who wish to up skill or change their domain to analytics and emerging technologies are free to apply.\\nSelection procedure includes\\n- Online Aptitude Test\\n- Logical Ability Test / Written Test\\nOn being shortlisted, you will be have to undergo a one-one discussion with our counsellor for further evaluation and processing of your Resume.\\nWhat you can expect from us?\\nYou will get trained on the following modules for a period of 12-14 weeks:\\n-SQL & PLSQL\\n-Data Wrangling using Python\\n-Statistics for Machine Learning,\\n-Artificial Intelligence, Data Interpretation\\n-Supervised & Unsupervised Learning,\\n-NLP & Deep Learning\\n-Cloud Data Lake\\n-Business intelligence & Data Visualization\\n-Simulation Projects\\nWhat is the expected Outcome?\\nAt the end of the Training you are expected to be well versed with the following:\\n- Analysis of large and complex data sets from multiple sources\\n- Development and evaluation of data analytics models, algorithms and solutions\\n- Understanding/implementation of ML algorithms, performance tuning and reporting\\n- Implementation of algorithms to mine targeted data and the ability to convert data into a business story\\n- Translation of business requirements into technical requirements; Data extraction, preparation and transformation\\n- Identification, development and implementation of statistical techniques and algorithms that address business challenges and adds value to the organisation\\n- Requirement Analysis and communication of findings in the form of a meaningful story with the stakeholders\\nCenter for Artificial Intelligence & Advanced Analytics (CAIA) focuses on the following:\\n1. Global Research on emerging trends, technologies and applications in AI and Advanced Analytics\\n2. Advanced Training programs for readying the future ready workforce\\n3. Solutions to herald the futuristic lifestyle and workspaces in the field of AI and Data Science.\\nhttp://www.centerforaia.com/\\nCenter for Artificial Intelligence and Advanced Analytics (Center for AIA) is the brainchild of experienced and visionary alumni of IIT Madras and Bombay. Digital leaders 5F World and Systech Solutions have joined hands to create a venture for architecting the future of society, workforce, governments and businesses. 5F World specializes in designing solutions around digital platforms and Systech Solutions has an expertise in architecting Artificial Intelligence and Advanced Analytics solutions for Fortune 500 companies through specialized programmed.\\n5F World\\n5F World is a leader in digital transformational journeys and has brought together the best minds in industry, academia and technology domains to develop a unique framework to transform stakeholder journeys through innovation and digitalization of businesses and education institutions.\\nSystech Solutions\\nSystech Solutions is a leading organisation in Data Strategy, Management & Analytics services provider with deep technology expertise and over 20 years of industry experience. Systech Solutions helps empower clients with innovative, data-driven solutions to reimagine their enterprise and has forged partnerships with industry-leading technology providers to develop a full spectrum of data services.\\n\\nWebsite\\nhttp://www.centerforaia.com/\\n\\nhttps://inflexion-analytix-private-limited.business.site/?m=true\\n\\n\\n\\n',\n",
       " 'About Data Sciences at Flipkart:\\nThe Data Sciences team at Flipkart is on a mission to build systemic intelligence across Flipkart products and the overarching ecosystem. Being India’s largest online marketplace and the most used e-commerce app in India, places Flipkart in a unique position and gives this team a distinctive opportunity — to decipher the richest possible data about Indian consumers. Add the dimension of a vast product selection and a proliferating seller base to that and what you get is a multitude of disruptive possibilities.\\n\\nIn a nutshell, the terabytes of daily data compounded in Flipkart’s data centers offer a dynamic mix of numerical, structured, unstructured, image- and audio-based statistics, all set to define shopping in the future.\\n\\nWhat this job entails: The pool of data available at Flipkart forms the foundation to solve some present and predictable challenges for shoppers in India. As a Data Scientist you will be working on:\\n• Product discovery along with personalization and intent modeling\\n• Demand shaping and planning\\n• Heterogeneous networks for consumer, product and seller interactions\\n• Customer insights\\n• Catalog enhancement and product insights\\n• Customer emotion detection and right response matching\\n• Fulfillment automation\\n• Optimization of last mile delivery\\n• Fraud modeling\\nAssociations and collaborations:\\nThe Flipkart Data Science team is also leveraging partnerships with Indian and foreign universities — like CMU, IITs, IISc and IIIT’s — for developing class-leading solutions in the e-commerce sector.\\n\\nAt Flipkart, the work of a Data Scientist involves collaboration with the engineering and product teams to ensure a holistic outcome at the product delivery. The flat functional structure within Flipkart engineering enables data scientists to focus on excellence and create a deep sense of ownership at every stage of work.\\n\\nWorking@Flipkart: Data Sciences techniques at Flipkart span classification, clustering, matrix factorization, graphical models, networks and graph algorithms, topic modeling, image processing, deep learning and NLP — each one of them being applied across large scale initiatives.\\n\\nIf you aspire to redefine ‘state-of-the-art’ and create an impact on India’s shopping landscape, Flipkart Data Science Organization offers the podium to solve challenging real-world problems and take a giant leap in your career as a Data Scientist.\\n\\nQualifications and experience: B.Tech/ M.Tech/ PhD in CS/ Statistics with demonstrable experience through publications/deployed solutions/projects. Experience of over 8 years.',\n",
       " 'Experience Required: 0 - 6 Years.\\nWe are looking for highly motivated and qualified researchers to be part of our team. This role is ideal for candidates with strong skills in research and technical implementation .\\nAs a part of the CVIP team, you are expected to:\\nBe thorough in the theory of image processing and computer vision: Image registration, image retrieval, multiple-view geometry and transformations, optimization and estimation, significant point detection and image descriptors.\\nBuild and deploy computer vision solutions either on the edge or cloud.\\nExperiment with new machine learning algorithms suitable for the business use cases.\\nAnalyze the performance of models and related business metrics and provide insights.\\nUnderstand client s business to be able to articulate the business problem, create relevant solutions using technical knowledge and deploy the solutions to drive revenue.\\nOptimize and tune the solutions/models for the client context.\\nCommunicate with end clients on solutions, recommendations and performance.\\nCollaborate and work closely with several key stakeholders (account, engineering, product and RD managers) to drive revenue generation.\\nEducational Qualifications:\\nMaster s or doctoral degree from reputed institutes like (IISc, IITs, IIITs or reputed international universities) with relevant RD experience in one of the following fields:\\nComputer Vision\\nImage processing\\nComputer Graphics and Augmented Reality\\nMachine Learning and Artificial intelligence\\nVideo processing/compression\\nDesired Hard Skills:\\n2 to 5 years experience in using computer vision and augmented reality toolkits e.g., OpenCV etc.\\nExperience with general purpose programming languages e.g., C/C , Java, Python.\\nExperience in Big Data technologies - Spark, Hadoop (Preferred).\\nExperience working with Databases and SQL Queries (Preferred).\\nDesired Soft Skills:\\nExcellent communication skills\\nAnalytical skills\\nGood presentation skills\\nAttention to details\\nWillingness to learn new technologies',\n",
       " '---',\n",
       " 'Sr. Data Scientist (Min 6 yrs in Data Science/ML)\\n\\nKey Responsibilities\\n• Help develop vision, business case, modeling spec and operational project plans for end-to-end ML solution\\n• In partnership with data leaders and ML engineering team, lead hands-on development of data science/machine learning models for high business impact opportunities across all business areas\\n• Work on all stages of a data science / ML project - exploration and conceptualization, POC (proof of concept), data preparation, model development and testing, deployment, monitoring and debugging, continuous improvement\\n• Partner with product, data engineering and ML engineering teams to ensure seamless productionzation of developed algorithm and management of full model cycle for business application\\n• Follow best practices for developing, managing, maintaining and documenting machine learning models and all related work\\n• Develop a deep understanding of different business functions and core KPI to ensure that the data science solution fits the business need\\n• Define and abstract standard building blocks for usage across multiple ML algorithms. Lead productization, development and maintenance of these. For example, feature library, model testing automation, model validation and recommendation automation\\n• Innovate new solutions for unique challenges facing Poshmark. Publish academic papers, white papers, blog posts and present in leading conferences.\\nQualifications And Skills\\n• 4+ years of hands-on experience in building scalable Machine Learning based solutions in a Big Data environment.\\n• Fluency in all steps of data science solution development cycle from business problem identification to business value delivery\\n• Ability to do insightful data analysis both pre and post ML model development to estimate impact, establish feasibility, understand model performance, validate hypothesis and answer questions from stakeholders\\n• Expert level hands-on / applied knowledge of key machine learning algorithms\\n• Ability to choose a technique based on the problem and the data, ability to debug and reason about why a model may not be working as expected\\n• Expert hands-on knowledge of Python and data science / ML packages\\n• Expert hands-on knowledge of SQL with the ability to write readable and efficient queries for complex data crunching\\n• Hands-on experience in using Spark (Scala or Pyspark) to process data at large scale\\n• Hands-on experience in using MS Excel to do quick exploratory data analysis using basic functions, pivot tables and charts\\n• Experience partnering with ML engineering teams in deploying ML models to production\\n• Strong business / product sense and excellent verbal and written communication skills for a variety of audiences: executive teams, product managers, engineers, and business leaders\\n• Excellent problem solving skills coupled with a passion to solve challenging problems using data science/ML with focus on delivering business value\\n• PhD or MS in a relevant quantitative field such as Statistics, Math, Computer Science is a huge plus\\n\\nQualification - Any\\n\\nWork Experience Required: 6-15 years',\n",
       " 'Our Exciting Opportunity\\n\\nWe are looking for Data Scientist for Bangalore and Chennai location-\\n\\nYou will\\nContribute to rapid and iterative development of high quality ML/AI applications. This includes working with petabytes of 4G/5G-networks, IoT and exogenous data, and proposing/selecting/testing predictive models, recommendation engines, anomaly detection systems, statistical model, deep learning, reinforcement learning and other machine learning systems.\\nWork with new technologies and champion them in MI Communities within Ericsson.\\nPresent and be prominent in MI related forums and conferences, e.g., presenting papers, organizing sessions and be a panelist\\nTo be successful in the role you must have\\nSolid skills in Machine Learning especially techniques such as Linear/Logistic Regression, Bagging, Bayesian model, Neural Networks, Random forest, Gradient boosting, Hyperparameter optimization techniques etc.\\nDemonstrable skills and track record (Github, open source etc.) in the use of current state of the art machine learning frameworks such as Keras, TensorFlow, Scikit-Learn, H2o, Spark etc. in developing ML/AI applications\\nProgramming skills in various languages (C++, Scala, Java, R) with proficiency in Python and/or C++',\n",
       " 'Roles and Responsibilities\\nScientist should have experience in machine learning algorithms, predictive analytics, demand forecasting. (S)He should have strong statistical background in descriptive and inferential statistics, regression, forecasting techniques. Good background in?data?structures, graph theory, object oriented programming. He should have Programming background in Python (including packages like Tensorflow), R, D3.js, Tableau, Spark, SQL, MongoDB.\\n\\nQualification: Background in a highly quantitative field like Data Science, Computer Science, Statistics, Applied Mathematics, Operations Research, Industrial Engineering, or similar fields.',\n",
       " '---',\n",
       " '---',\n",
       " \"Roles and Responsibilities\\n\\nIntroduction\\nAs a Data Scientist at IBM, you will help transform our clients’ data into tangible business value by analyzing information, communicating outcomes and collaborating on product development. Work with Best in Class open source and visual tools, along with the most flexible and scalable deployment options. Whether it’s investigating patient trends or weather patterns, you will work to solve real world problems for the industries transforming how we live.\\n\\n\\nYour Role and Responsibilities\\nAs an Advisory Data Scientist, you will demonstrate expertise in Advanced people analytics. We use the latest advanced analytics technologies, tools, and techniques to extract insights from our data. We work collaboratively with other teams within the company and apply these insights to make complex and strategic decisions for the benefit of IBM. As a Data Scientist, you'll team with some of the best mind in the industry to create innovative world class solutions focused on business needs. You'll do this by designing/developing analytical approaches to solve our client's business problems.\\n\\nResponsibilities:\\nWork with clients in defining the key business problems to be solved while developing, maintaining and leveraging key client relationships.\\nIdentifying, gathering, and analyzing complex, multi-dimensional datasets utilizing a variety of tools.\\nDesigning Advanced Analytics solutions and services in the area(s) of optimization (linear, mixed integer, constraint programming) or simulations, or predictive analytics.\\nFormulating mathematical approaches to solve those problems potentially utilizing ILOG CPLEX, Standard Query Language (SQL), Apache Spark, Statistical Packages Social Sciences(SPSS), Python, R.\\nUsing industry technologies, tools and data mining frameworks for data analytics including data visualization for analyzing and drawing conclusions.\\nCommunicating the data insights to key stakeholders and get their buy-ins.\\nIntegrating the analytics Into existing business work flows.\\nMeasuring business outcome (quantitatively and/or qualitatively) on a periodic basis.\\nDeveloping junior team members and leading project tasks.\\nMaintaining knowledge and understanding of current and emerging trends within the analytics industry.\\nA good level of business acumen and business domain knowledge is highly preferred\\n\\n\\nRequired Technical and Professional Expertise\\nA minimum of 2 years of experience in determining mathematical approaches to solve problems, sampling plan and gathering/analyzing /portraying data\\nProficient in designing/building/managing solutions utilizing for example ILOG CPlex, Python, Standard Query Language (SQL), Apache Spark, Statistical Package Social Sciences(SPSS), or R\\nWork experience in building predictive models using various ML algorithms, drawing insights from the model output, communicating the results to key stakeholders, integrating the analytics into existing business workflows and measuring business outcome\\nExpertise in statistical analysis and deploying the results of the analysis\\nExperience in data collection/data mining/text mining\\n\\nPreferred Technical and Professional Expertise\\nA minimum of 5 years experience in determining mathematical approaches to solve problems, sampling plan and gathering/analyzing /portraying data\\nExposure to unstructured data sets and Natural Language Processing\\nProven Business acumen and business domain knowledge is highly preferred\"]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the\n",
    "webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering “Data Scientist” in “Skill,Designations,Companies” field \n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the location check box\n",
    "loc=driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\")\n",
    "\n",
    "# clicking the check box\n",
    "loc.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the salary check box\n",
    "loc=driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\")\n",
    "\n",
    "# clicking the check box\n",
    "loc.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Only Fresher / Data Scientist / Data Analyst / Business Analytics- MNC',\n",
       " 'Only Fresher / Data Scientist / Data Analyst / Business Analytics- MNC']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        job_title.append(\"--\") \n",
    "    else:\n",
    "        job_title.append(i.text)\n",
    "    \n",
    "job_title[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Delhi NCR, Ghaziabad, Gurgaon', 'Faridabad, Delhi NCR, Greater Noida']"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the job-location\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in locations:\n",
    "    if i.text is None :\n",
    "        job_location.append(\"--\") \n",
    "    else:\n",
    "        job_location.append(i.text)\n",
    "job_location[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-0 Yrs', '0-0 Yrs']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scraping the company_name \n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\") \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "company_name[0:2]\n",
    "\n",
    " #scraping the experience_required \n",
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience:\n",
    "    if i.text is None :\n",
    "            experience_required.append(\"--\") \n",
    "    else:\n",
    "            experience_required.append(i.text)\n",
    "experience_required[0:2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_required</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Delhi NCR, Ghaziabad, Gurgaon</td>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Faridabad, Delhi NCR, Greater Noida</td>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>WellMed Medical Management</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Associate Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-5 Yrs</td>\n",
       "      <td>AlgoScale Technologies Private Limited</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>(3 Reviews)</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Data Scientist/Analyst - Machine Learning/Deep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_required                            company_name  \\\n",
       "0             0-0 Yrs               GABA Consultancy services   \n",
       "1             0-0 Yrs               GABA Consultancy services   \n",
       "2             4-8 Yrs              WellMed Medical Management   \n",
       "3             1-5 Yrs  AlgoScale Technologies Private Limited   \n",
       "4             3-6 Yrs                             (3 Reviews)   \n",
       "\n",
       "                          job_location  \\\n",
       "0        Delhi NCR, Ghaziabad, Gurgaon   \n",
       "1  Faridabad, Delhi NCR, Greater Noida   \n",
       "2                                Noida   \n",
       "3                                Noida   \n",
       "4                                Delhi   \n",
       "\n",
       "                                           job_title  \n",
       "0  Only Fresher / Data Scientist / Data Analyst /...  \n",
       "1  Only Fresher / Data Scientist / Data Analyst /...  \n",
       "2                           Associate Data Scientist  \n",
       "3                                     Data Scientist  \n",
       "4  Data Scientist/Analyst - Machine Learning/Deep...  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"experience_required\":experience_required[0:10],\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_title\":job_title[0:10]})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location. You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "WEB SCRAPING ASSIGNMENT-2\n",
    ".\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.glassdoor.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your login credentials in the chrome window opened by webdriver. Do this step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering “Data Scientist” in “Skill,Designations,Companies” field \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='sc.keyword']\"))).send_keys(\"Data Scientist\")\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='sc.location']\"))).send_keys(\"Noida\")\n",
    "\n",
    "\n",
    "#search_field_designation=driver.find_element_by_xpath(\"//input[@id='sc.keyword']\")\n",
    "#search_field_designation.send_keys(\"Data Scientist\")\n",
    "#search_field_location=driver.find_element_by_xpath(\"//input[@id='sc.location']\")\n",
    "#search_field_location.send_keys(\"Noida\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//button[@data-test='search-bar-submit']\"))).click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists for scraping data\n",
    "\n",
    "company_ratings=[]\n",
    "company_name=[]\n",
    "days_ago=[]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xtLytics', 'xtLytics']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where company name is present\n",
    "companies=driver.find_elements_by_xpath(\"//a[@class=' css-10l5u4p e1n63ojh0 jobLink']/span\")\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\") \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "company_name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '3']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where ratings of the company  is present\n",
    "ratings=driver.find_elements_by_xpath(\"//span[@class='compactStars ']\")\n",
    "for i in ratings:\n",
    "    if i.text is None :\n",
    "        company_ratings.append(\"--\") \n",
    "    else:\n",
    "        company_ratings.append(i.text)\n",
    "company_ratings[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '2']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where  No. of days ago when job was posted is present\n",
    "days=driver.find_elements_by_xpath(\"//div[@data-test='job-age']\")\n",
    "for i in days:\n",
    "    if i.text is None :\n",
    "        days_ago.append(\"--\") \n",
    "    else:\n",
    "        days_ago.append(i.text[0])\n",
    "days_ago[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_ratings</th>\n",
       "      <th>days_ago</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xtLytics</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RiseIn Technologies Pvt Ltd</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unyscape Infocom Pvt. Ltd</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  company_name company_ratings days_ago\n",
       "0                     xtLytics               3        2\n",
       "1                     xtLytics               3        2\n",
       "2  RiseIn Technologies Pvt Ltd             3.1        3\n",
       "3    Unyscape Infocom Pvt. Ltd             3.9        1\n",
       "4                     Ericsson               4        2"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"company_name\":company_name[0:10],\"company_ratings\":company_ratings[0:10],\"days_ago\":days_ago[0:10]\n",
    "                })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation\n",
    "in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min\n",
    "salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company\n",
    "name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe.\n",
    "Note that all of the above steps have to be done by coding only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.glassdoor.co.in/Salaries/index.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering “Data Scientist” in “Skill,Designations,Companies” field \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='KeywordSearch']\"))).send_keys(\"Data Scientist\")\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='LocationSearch']\"))).send_keys(\"Noida\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,\"//button[@data-test='search-bar-submit']\"))).click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salaries=[]\n",
    "max_salaries=[]\n",
    "average_salaries=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹706K', '₹572K']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where min salary   is present\n",
    "min_salary=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[1]\")\n",
    "for i in min_salary:\n",
    "    if i.text is None :\n",
    "        min_salaries.append(\"--\") \n",
    "    else:\n",
    "        \n",
    "        min_salaries.append(i.text)\n",
    "min_salaries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹11,513K', '₹1,300K']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where max salary   is present\n",
    "max_salary=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")\n",
    "for i in max_salary:\n",
    "    if i.text is None :\n",
    "        max_salaries.append(\"--\") \n",
    "    else:\n",
    "        max_salaries.append(i.text)\n",
    "max_salaries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹11,513K', '₹1,300K']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where average salary   is present\n",
    "average_salary=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container ']/span[2]\")\n",
    "for i in average_salary:\n",
    "    if i.text is None :\n",
    "        average_salaries.append(\"--\") \n",
    "    else:\n",
    "        average_salaries.append(i.text)\n",
    "average_salaries[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Delhivery', 'Accenture']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the element where company_name   is present\n",
    "company=driver.find_elements_by_xpath(\"//div[@data-test='job-info']/p[2]\")\n",
    "for i in company:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\")      \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "company_name[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"company_name\":company_name[0:10],\"\":company_ratings[0:10],\"days_ago\":days_ago[0:10]\n",
    "                })\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to\n",
    "scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount \n",
    "To scrape \n",
    "the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and\n",
    "more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page\n",
    "you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of\n",
    "the page , then click on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialising driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element_by_class_name('LM6RPg')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys('sunglasses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the button and clicking it toh search for sunglasses\n",
    "button=driver.find_element_by_class_name('vh79eN')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the required details\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 3 page\n",
    "    brands=driver.find_elements_by_class_name('_2B_pmu')#scraping brands name by class name='_2B_pmu'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements_by_class_name('_2mylT6')#scraping description by class name = '_2mylT6'\n",
    "    for i in desc:\n",
    "        description.append(i.get_attribute('title'))#appending the description in list\n",
    "    prices=driver.find_elements_by_xpath(\"//div[@class='_1vC4OE']\")# scraping the price from the xpath\n",
    "    for i in prices[:40]:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements_by_xpath(\"//div[@class='VGWI6T']\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_2Xp0TH']\")#scraping the list of buttons from the page\n",
    "    driver.get(nxt_button[page].get_attribute('href'))#getting the link from the list for next page\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Brand                                        Description  \\\n",
      "0             Levi's                     Others Aviator Sunglasses (58)   \n",
      "1             Fossil  Mirrored, UV Protection Rectangular Sunglasses...   \n",
      "2           Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
      "3             Collet  Gradient, Mirrored, UV Protection Aviator Sung...   \n",
      "4          Elligator                UV Protection Round Sunglasses (54)   \n",
      "..               ...                                                ...   \n",
      "95              INSH  UV Protection, Night Vision, Riding Glasses Re...   \n",
      "96          Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
      "97  shah collections         UV Protection Round Sunglasses (Free Size)   \n",
      "98    kingsunglasses  Mirrored, UV Protection Wayfarer, Wayfarer, Wa...   \n",
      "99            Fossil                 Others Rectangular Sunglasses (54)   \n",
      "\n",
      "     Price Discount  \n",
      "0   ₹5,180  30% off  \n",
      "1   ₹2,420  45% off  \n",
      "2     ₹674  25% off  \n",
      "3     ₹199  86% off  \n",
      "4     ₹312  87% off  \n",
      "..     ...      ...  \n",
      "95    ₹327  86% off  \n",
      "96    ₹719  20% off  \n",
      "97    ₹331  80% off  \n",
      "98    ₹269  82% off  \n",
      "99  ₹2,200  50% off  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "#printing dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8:Scrape data for first 100 sneakers you find when you visit flipkart.comand search for “sneakers” in the search field.You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialising driver\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar=driver.find_element_by_class_name('LM6RPg')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys('sneakers')\n",
    "button=driver.find_element_by_class_name('vh79eN')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping the required details\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 3 page\n",
    "    brands=driver.find_elements_by_class_name('_2B_pmu')#scraping brands name by class name='_2B_pmu'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements_by_class_name('_2mylT6')#scraping description by class name = '_2mylT6'\n",
    "    for i in desc:\n",
    "        description.append(i.get_attribute('title'))#appending the description in list\n",
    "    prices=driver.find_elements_by_xpath(\"//div[@class='_1vC4OE']\")# scraping the price from the xpath\n",
    "    for i in prices[:40]:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements_by_xpath(\"//div[@class='VGWI6T']\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_2Xp0TH']\")#scraping the list of buttons from the page\n",
    "    driver.get(nxt_button[page].get_attribute('href'))#getting the link from the list for next page\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Brand                                        Description   Price  \\\n",
      "0      WROGN                                   Sneakers For Men  ₹1,424   \n",
      "1      WROGN                                   Sneakers For Men  ₹1,349   \n",
      "2     Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...    ₹461   \n",
      "3     Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...    ₹236   \n",
      "4     Chevit  Chevit Latest Fashion Combo Pack of 2 Pairs Ca...    ₹525   \n",
      "..       ...                                                ...     ...   \n",
      "95    Rontex                                   Sneakers For Men    ₹426   \n",
      "96  Claptrap  Mesh Walking Casual Sneakers Shoes for Men And...    ₹449   \n",
      "97  PERY-PAO                White Casual Shoes Sneakers For Men    ₹425   \n",
      "98   Shoefly  Combo Men Pack of 2 Loafers Shoes Sneakers For...    ₹331   \n",
      "99  Red Tape                                   Sneakers For Men  ₹1,023   \n",
      "\n",
      "   Discount  \n",
      "0   50% off  \n",
      "1   50% off  \n",
      "2   76% off  \n",
      "3   52% off  \n",
      "4   64% off  \n",
      "..      ...  \n",
      "95  57% off  \n",
      "96  10% off  \n",
      "97  57% off  \n",
      "98  66% off  \n",
      "99  75% off  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "#printing dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10:  Go to webpage https://www.amazon.in/Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”\n",
    "After setting the filters scrape first 10 laptops data.You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty list\n",
    "Title=[]\n",
    "Ratings=[]\n",
    "price=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialising driver\n",
    "#driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get(' https://www.amazon.in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the search bar\n",
    "search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")    # Locating searc_bar by id\n",
    "search_bar.clear()                                               # clearing search_bar\n",
    "search_bar.send_keys(\"laptops\")                                   # sending user input to search bar\n",
    "search_button = driver.find_element_by_xpath('//span[@id=\"nav-search-submit-text\"]')       # Locating search_button by xpath\n",
    "search_button.click()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the core i7 filter\n",
    "filter_button=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter_button:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the core i9 filter\n",
    "filter_button=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter_button:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Scrapping Titles\n",
    "titles=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in titles[:10]:\n",
    "    Title.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping Price\n",
    "prices=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in prices[:10]:\n",
    "    price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating Ratings\n",
    "urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")#collecting urls of all the laptop\n",
    "UR=[]\n",
    "for i in urls[:10]:\n",
    "    UR.append(i.get_attribute('href'))#getting the url of first 10 laptops\n",
    "for url in UR:#loop for every laptop in the list\n",
    "    driver.get(url)\n",
    "    try:                  #exception handling for nosuchelementexception\n",
    "        rate=driver.find_element_by_xpath(\"//span[@id='acrCustomerReviewText']\")#locating the ratingd link\n",
    "        rate.click()                                                      #click the rating link found\n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")#locating the rating\n",
    "        Ratings.append(rating.text)#appending the ratings in Ratings list\n",
    "        \n",
    "    except NoSuchElementException   as e:\n",
    "        Ratings.append(\"NO rating\")#appending the No rating if no rating is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#checking the length of each list\n",
    "print(len(Title))\n",
    "print(len(price))\n",
    "print(len(Ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title   Price       Ratings\n",
      "0  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  81,990  4.1 out of 5\n",
      "1  Dell Inspiron 5370 13.3-inch FHD Laptop (Core ...  68,490  4.1 out of 5\n",
      "2  (Renewed) Dell Inspiron 3567 Laptop Core i3-7t...  37,900     NO rating\n",
      "3  (Renewed) Dell Latitude E5570 15-inch Laptop (...  61,999     NO rating\n",
      "4  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  82,789  3.8 out of 5\n",
      "5  (Renewed) Dell Latitude E7240 12.5-inch Laptop...  39,999  3.6 out of 5\n",
      "6  Lenovo Legion 5i 10th Gen Intel i7 15.6\" FHD G...  81,489  4.1 out of 5\n",
      "7  (Renewed) HP EliteBook 1030 G2 X 360 Notebook ...  49,990  2.2 out of 5\n",
      "8  Dell 2019 Dell Inspiron 14 5482 14 Inch FHD 2-...  57,490    4 out of 5\n",
      "9  (Renewed) HP EliteBook X360 1030 G2 Laptop (Co...  49,990  2.6 out of 5\n"
     ]
    }
   ],
   "source": [
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Title':Title,\n",
    "                'Price':price,\n",
    "                'Ratings':Ratings})\n",
    "#printing dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9.\"https://www.myntra.com/shoes\". Set price filter \"Rs 6649 to Rs 13099\" and color filter to \"Black\" and then scrap 100 shoes data. The data should include \"Brand\" of shoes, shoe short-description and price. Please not: Everything should done through code even the filtering for sneakers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the required libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0%2C6337.0_10225.0_6337.0%20TO%2010225.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Brand                         Short-description  \\\n",
      "0             Reebok            Harmony 3.5 Road Running Shoes   \n",
      "1               Puma            Trailfox Overland MTS Grid Run   \n",
      "2             ADIDAS                 Men Questar Flow Sneakers   \n",
      "3               Geox               Women Solid Leather Loafers   \n",
      "4               Nike                    Men Zoom Running Shoes   \n",
      "5             ADIDAS               Men FluidFlow Running Shoes   \n",
      "6               Nike              Men AIR MAX Basketball Shoes   \n",
      "7     ROSSO BRUNELLO  Men Solid Italian Leather Formal Loafers   \n",
      "8               ALDO              Women Leather Open Toe Flats   \n",
      "9             Reebok                 Men Nano X Training Shoes   \n",
      "10              Nike                    Men Zoom Running Shoes   \n",
      "11              Geox                 Men Leather Formal Derbys   \n",
      "12              Geox                    Women Leather Slip-Ons   \n",
      "13            ADIDAS               Men Galaxar Run Sustainable   \n",
      "14      Kenneth Cole           Men Solid Leather Formal Derbys   \n",
      "15              Puma                         Men Running Shoes   \n",
      "16              Geox                       Men Leather Loafers   \n",
      "17              ALDO          Men Solid Leather Formal Oxfords   \n",
      "18            ADIDAS               Men Dame 6 Basketball Shoes   \n",
      "19              Geox               Men Leather Formal Slip-Ons   \n",
      "20              Geox                 Men Leather Driving Shoes   \n",
      "21              Nike                  Women ZOOM Running Shoes   \n",
      "22            ADIDAS            Men Edge Gameday Running Shoes   \n",
      "23          Skechers             Men Go Run Ride Running Shoes   \n",
      "24      Hush Puppies         Men Solid Leather Formal Slip-Ons   \n",
      "25              Geox                 Men Leather Driving Shoes   \n",
      "26            DIESEL                     Men DANNY LC Sneakers   \n",
      "27            ADIDAS                 Men Energy Falcon Running   \n",
      "28              Puma             Men SPEED 500 2 Running Shoes   \n",
      "29              Geox               Men Leather Formal Slip-Ons   \n",
      "30              ALDO                         Men Solid Loafers   \n",
      "31            Clarks                Men Leather Formal Brogues   \n",
      "32              Geox                Men Solid Leather Sneakers   \n",
      "33              Geox                  Women Leather Boat Shoes   \n",
      "34  ADIDAS Originals                    Men SEELEY Skate Shoes   \n",
      "35              Geox                 Men Leather Formal Derbys   \n",
      "36              ALDO              Men Textured Leather Loafers   \n",
      "37      UNDER ARMOUR             Charged Aurora Training Shoes   \n",
      "38         Cole Haan               Men Wingtip Oxford Sneakers   \n",
      "39              Xtep                         Men Running Shoes   \n",
      "40              Geox                  Women Sequinned Sneakers   \n",
      "41              Geox                Men Woven Slip-On Sneakers   \n",
      "42      UNDER ARMOUR            Charged Bandit 5 Running Shoes   \n",
      "43            DIESEL                      Men Mid-Top Sneakers   \n",
      "44         Cole Haan                          Leather Sneakers   \n",
      "45              Geox                Men Leather Formal Loafers   \n",
      "46         Cole Haan       Men Traveller Leather Penny Loafers   \n",
      "47              Xtep                         Men Running Shoes   \n",
      "48            Reebok                   Men HIIT Training Shoes   \n",
      "49              ALDO                       Men Leather Loafers   \n",
      "\n",
      "                              Price  \n",
      "0                         Rs. 11999  \n",
      "1    Rs. 6999Rs. 9999(Rs. 3000 OFF)  \n",
      "2                          Rs. 6999  \n",
      "3                          Rs. 7999  \n",
      "4                          Rs. 7995  \n",
      "5                          Rs. 7999  \n",
      "6                          Rs. 7995  \n",
      "7        Rs. 7699Rs. 10999(30% OFF)  \n",
      "8                          Rs. 6999  \n",
      "9                          Rs. 9999  \n",
      "10                         Rs. 7995  \n",
      "11                        Rs. 12990  \n",
      "12                         Rs. 8990  \n",
      "13   Rs. 6399Rs. 7999(Rs. 1600 OFF)  \n",
      "14        Rs. 6643Rs. 9490(30% OFF)  \n",
      "15                         Rs. 6999  \n",
      "16                        Rs. 10990  \n",
      "17  Rs. 6999Rs. 13999(Rs. 7000 OFF)  \n",
      "18  Rs. 8199Rs. 10999(Rs. 2800 OFF)  \n",
      "19                         Rs. 9490  \n",
      "20                         Rs. 7999  \n",
      "21                         Rs. 7995  \n",
      "22   Rs. 6399Rs. 7999(Rs. 1600 OFF)  \n",
      "23   Rs. 7899Rs. 8999(Rs. 1100 OFF)  \n",
      "24        Rs. 7649Rs. 8999(15% OFF)  \n",
      "25                         Rs. 9499  \n",
      "26       Rs. 9093Rs. 13990(35% OFF)  \n",
      "27                         Rs. 6999  \n",
      "28   Rs. 6999Rs. 9999(Rs. 3000 OFF)  \n",
      "29                        Rs. 12990  \n",
      "30                        Rs. 11999  \n",
      "31                         Rs. 7999  \n",
      "32                        Rs. 10999  \n",
      "33                         Rs. 8990  \n",
      "34                         Rs. 6999  \n",
      "35                        Rs. 12990  \n",
      "36                        Rs. 12999  \n",
      "37                         Rs. 6999  \n",
      "38                        Rs. 12999  \n",
      "39       Rs. 9899Rs. 10999(10% OFF)  \n",
      "40       Rs. 6599Rs. 10999(40% OFF)  \n",
      "41                        Rs. 10990  \n",
      "42                         Rs. 7999  \n",
      "43     Rs. 7800Rs. 13000( 40 % OFF)  \n",
      "44                        Rs. 10999  \n",
      "45                         Rs. 8499  \n",
      "46       Rs. 8999Rs. 19999(55% OFF)  \n",
      "47        Rs. 6569Rs. 7299(10% OFF)  \n",
      "48   Rs. 7199Rs. 8999(Rs. 1800 OFF)  \n",
      "49                        Rs. 12999  \n"
     ]
    }
   ],
   "source": [
    "start_page=0\n",
    "end_page=0\n",
    "for page in range(start_page,end_page+1): \n",
    "    nxt_button=driver.find_element_by_xpath(\"//li[@class='pagination-next']\")\n",
    "    \n",
    "    #creating empty lists\n",
    "    shoe_names=[]\n",
    "    s_desc=[]\n",
    "    short_desc=[]\n",
    "    price=[]\n",
    "    \n",
    "    #for scrapping shoe brand names\n",
    "    Names=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h3\")\n",
    "    for i in Names:\n",
    "        shoe_names.append(i.text)\n",
    "    \n",
    "    #for scrapping shoe short-description\n",
    "    desc=driver.find_elements_by_xpath(\"//div[@class='product-productMetaInfo']/h4\")\n",
    "    for i in desc:\n",
    "        s_desc.append(i.text)\n",
    "    #As, the s_desc list contain blank description in every alternate index, so removing the blank or null description\n",
    "    for j in range(0,len(s_desc),2):\n",
    "        short_desc.append(s_desc[j])\n",
    "    \n",
    "    #for scrapping shoe prices\n",
    "    desc=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "    for i in desc:\n",
    "        price.append(i.text)\n",
    "    \n",
    "    #for scrapping the datas from next pages.\n",
    "    if nxt_button.text=='next':\n",
    "            nxt_button.click()\n",
    "            time.sleep(5)\n",
    "            \n",
    "    #creating a dataframe to store above scraped details\n",
    "    df=pd.DataFrame({'Brand': shoe_names,\n",
    "                    'Short-description': short_desc,\n",
    "                    'Price': price})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7 flipkart iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer starts from here\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\User 14\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Creating empty list\n",
    "urls=[]\n",
    "short_desc=[]\n",
    "desc=[]\n",
    "stars=[]\n",
    "\n",
    "#Taking 10 pages into consideration using for loop\n",
    "for i in range(10):\n",
    "    url=driver.find_element_by_xpath(\"//a[@class='_2Xp0TH']\").get_attribute('href')\n",
    "    driver.get(url)\n",
    "    #for scrapping the number of stars\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='col _390CkK _1gY8H-']/div[1]/div[1]\"):\n",
    "        stars.append(j.text)\n",
    "    #for scrapping the short review\n",
    "    for k in driver.find_elements_by_xpath(\"//p[@class='_2xg6Ul']\"):\n",
    "        short_desc.append(k.text)\n",
    "    #for scrapping the complete review\n",
    "    for l in driver.find_elements_by_xpath(\"//div[@class='qwjRop']/div/div\"):\n",
    "        desc.append(l.text)\n",
    "#printing the length of lists\n",
    "print(len(stars))\n",
    "print(len(short_desc))\n",
    "print(len(desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of Stars      Short Description  \\\n",
      "0                5      Terrific purchase   \n",
      "1                5              Wonderful   \n",
      "2                5          Great product   \n",
      "3                5  Mind-blowing purchase   \n",
      "4                5      Terrific purchase   \n",
      "..             ...                    ...   \n",
      "95               5      Worth every penny   \n",
      "96               5      Worth every penny   \n",
      "97               5       Perfect product!   \n",
      "98               4           Nice product   \n",
      "99               5              Wonderful   \n",
      "\n",
      "                                          Description  \n",
      "0   Upgraded from iphone 6 to 11 best phone for ip...  \n",
      "1   This is my first ever I phone. Before this I w...  \n",
      "2   Well you all know the specifications . One of ...  \n",
      "3   This will help you more. See if you are planni...  \n",
      "4   The built quality is not very premium.\\nThe ba...  \n",
      "..                                                ...  \n",
      "95  Best budget Iphone till date ❤️ go for it guys...  \n",
      "96  It’s been almost a month since I have been usi...  \n",
      "97  Iphone is just awesome.. battery backup is ver...  \n",
      "98  Awesome Phone. Slightly high price but worth. ...  \n",
      "99  *Review after 10 months of usage*\\nDoesn't see...  \n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combining all the lists into a single dataframe\n",
    "df=pd.DataFrame({'Number of Stars': stars,\n",
    "                'Short Description': short_desc,\n",
    "               'Description': desc})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
